{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHKzEhWtPrLJ"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "'''import aiohttp\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()'''\n",
        "\n",
        "#nltk.download('stopwords',download_dir = '/root/nltk_data')\n",
        "#nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urldata = pd.read_csv('/content/knu_students.csv')[['KboNr','URL','NACE']].head(1400)\n",
        "textdata = pd.DataFrame({'KboNr':urldata['KboNr'],'Text':[None] * len(urldata),'NACE':urldata['NACE']})"
      ],
      "metadata": {
        "id": "P1nCCvXCImOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_website(url, original_url = True):\n",
        "    try:\n",
        "        response = requests.get(str(url))\n",
        "        if response.status_code == 200:\n",
        "            #soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            text = soup.get_text()\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "            return text\n",
        "        else:\n",
        "            if original_url:\n",
        "              add_url_error(url, response.status_code)\n",
        "              print(f\"Failed to retrieve content from {url}. Status code: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_domain(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    domain = f\"{parsed_url.scheme}://{parsed_url.netloc}/\"\n",
        "    return domain\n",
        "\n",
        "def add_url_error(url, status_code):  #This is to check which errors occur and how often.\n",
        "                                      #The URL get's added to the list now only for troubleshooting.\n",
        "    error_urls[status_code].append(url)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_url(row, languages=['nl'], min_word_count=20): #Voor engels moeten we ook nog een oplossing vinden om de site naar nederlands te brengen over te slaan.\n",
        "                                                                  #Engelse text mogen we niet scrapen.\n",
        "    url = row.URL  # Extract the URL from the row\n",
        "    url = get_domain(url)\n",
        "    original_text = scrape_website(url)\n",
        "\n",
        "    if original_text:\n",
        "        language = detect(original_text)\n",
        "        #if language != f'nl':\n",
        "        if language not in languages:\n",
        "          if language == f'fr':\n",
        "            attempt_nr = 0 #attempt_nr to keep track of how many times URL conversion has been attempted\n",
        "                            # This way we can use multiple methods until one works.\n",
        "            try_new_url = True #Set to true so we can stop when we find dutch site\n",
        "\n",
        "            while try_new_url:\n",
        "              new_url, attempt_nr = convert_url_fr(url, attempt_nr) #Try to convert URL and keep track of attempt number.\n",
        "              new_text = scrape_website(new_url, False)\n",
        "              new_language = detect(original_text)\n",
        "\n",
        "              if new_language == f'nl':\n",
        "                original_text == new_text\n",
        "                try_new_url = False\n",
        "\n",
        "              elif attempt_nr == 0:\n",
        "                try_new_url = False\n",
        "                print(f\"Language error: {new_language} Failed to retrieve content from {url}\")\n",
        "                return None\n",
        "\n",
        "          else: #If language is not dutch and not french\n",
        "              print(f\"Language error: {language} Failed to retrieve content from {url}\")\n",
        "              return None\n",
        "\n",
        "        if len(original_text.split()) >= min_word_count:# and language in languages:\n",
        "            return original_text\n",
        "\n",
        "        elif len(original_text.split()) < min_word_count:\n",
        "                print(f\"Text from {url} doesn't meet the word count criteria (actual count: {len(original_text.split())}, required count: {min_word_count})\")\n",
        "\n",
        "        else:\n",
        "             print(f\"Unkown Problem {url}\")\n",
        "             return None\n",
        "    else:\n",
        "        print(f\"Failed to retrieve content from {url}\")\n",
        "        return None\n",
        "\n",
        "def convert_url_fr(url, attempt_nr):\n",
        "  if attempt_nr == 0:\n",
        "    attempt_nr += 0\n",
        "    if ('/fr' in url):\n",
        "        return url.replace('/fr','/nl'), attempt_nr\n",
        "    if ('/en' in url):\n",
        "      return url.replace('/en','/nl'), attempt_nr\n",
        "    if ('/de' in url):\n",
        "        return url.replace('/de','/nl'), attempt_nr\n",
        "    elif (url[-1:] == '/'):\n",
        "        return url + 'nl', attempt_nr\n",
        "    elif (url[-1:] != '/'):\n",
        "        return url + '/nl', attempt_nr\n",
        "    #skip these steps at the moment.\n",
        "    '''elif attempt_nr == 1: #?lang=fr\n",
        "    attempt_nr = 0\n",
        "    if ('lang=fr' in url):\n",
        "        return url.replace('/lang=fr','/lang=nl'), attempt_nr\n",
        "    if ('lang=en' in url):\n",
        "        return url.replace('/lang=en','/lang=nl'), attempt_nr\n",
        "    if ('lang=de' in url):\n",
        "        return url.replace('/lang=de','/lang=nl'), attempt_nr\n",
        "    elif (url[-1:] == '/'):\n",
        "        return url + '?lang=nl', attempt_nr\n",
        "    elif (url[-1:] != '/'):\n",
        "        return url + '?lang=nl', attempt_nr\n",
        "\n",
        "    elif attempt_nr == 2: #?lang=fr\n",
        "      attempt_nr = 0'''\n",
        "\n",
        "    parsed_url = urlparse(url)\n",
        "    # Construct the new URL with the language code as a subdomain\n",
        "    new_url = f\"{parsed_url.scheme}://nl.{parsed_url.netloc}{parsed_url.path}\"\n",
        "    return new_url, attempt_nr\n"
      ],
      "metadata": {
        "id": "UPd7OM4iENZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Set the maximum number of threads (adjust as needed)\n",
        "    max_threads = 35\n",
        "    error_urls = {\n",
        "      100: [],  101: [],  102: [],\n",
        "      200: [],  201: [],  202: [],  203: [],  204: [],  205: [],  206: [],\n",
        "      300: [],  301: [],  302: [],  303: [],  304: [],  305: [],  306: [],  307: [],  308: [],\n",
        "      400: [],  401: [],  402: [],  403: [],  404: [],  405: [],  406: [],  407: [],  408: [],\n",
        "      409: [],  410: [],  411: [],  412: [],  413: [],  414: [],  415: [],  416: [],  417: [],\n",
        "      418: [],  421: [],  422: [],  423: [],  424: [],  425: [],  426: [],  428: [], 429: [],\n",
        "      431: [], 451: [],\n",
        "      500: [], 501: [], 502: [], 503: [], 504: [], 505: [], 506: [], 507: [], 508: [],\n",
        "      510: [], 511: []\n",
        "      }\n",
        "\n",
        "    with ThreadPoolExecutor(max_threads) as executor:\n",
        "\n",
        "        #loop = asyncio.get_event_loop()\n",
        "        #tasks = [loop.run_in_executor(executor, process_url, url) for url in urldata]\n",
        "        #tasks = [loop.process_url(url) for url in urldata]\n",
        "        #results = await asyncio.gather(*tasks)\n",
        "\n",
        "        results = list(executor.map(process_url, urldata.itertuples(index=False)))\n",
        "\n",
        "\n",
        "    # Assign values to the corresponding columns in textdata\n",
        "    for i, result in enumerate(results):\n",
        "        if result is not None:\n",
        "            textdata.at[i, 'Text'] = result\n",
        "        else:\n",
        "            textdata.at[i, 'Text'] = None\n",
        "\n",
        "    textdata.to_csv('/content/textdata.csv')\n",
        "\n",
        "    for key in error_urls.keys():\n",
        "      if len(error_urls[key]) != 0:\n",
        "        print(f'Error code {key}: {error_urls[key]}')\n",
        "\n",
        "\n",
        "    print(textdata['Text'].count())\n",
        "    print(textdata['KboNr'].count())"
      ],
      "metadata": {
        "id": "m-2AmSZN-PHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "'''loop = asyncio.get_event_loop()\n",
        "task = asyncio.ensure_future(main())\n",
        "loop.run_until_complete(asyncio.sleep(2))'''"
      ],
      "metadata": {
        "id": "e4lnpyB7j3fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S3Yo_bwSwyym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runtime 1375 urls: 6m50-7m10s\n",
        "Runtime (est) 260.000 urls: 21,5h22,6h\n",
        "Succes 1375 urls: 673\n",
        "Succes (est) 260.000 urls: 190.000\n",
        "Data size 1375 urls: 2,3MB\n",
        "Data size (est) 260.000 urls: 800MB\n",
        "\n"
      ],
      "metadata": {
        "id": "oc95OSbHwy4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "                '''elif language not in languages:\n",
        "                nl_url = convert_url_fr(url)\n",
        "                nl_text = scrape_website(nl_url)'''\n",
        "\n",
        "                '''elif nl_text and len(nl_text.split()) >= min_word_count:\n",
        "                return nl_text, nl_text\n",
        "                tokens = word_tokenize(nl_text)\n",
        "                stop_words = set(stopwords.words(\"dutch\"))\n",
        "                filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "                words_to_be_removed = [\"cookie\", \"cookies\", \"tracking\", \"privacy\"]\n",
        "                filtered_tokens = [word for word in filtered_tokens if word.lower() not in words_to_be_removed]\n",
        "\n",
        "                stemmer = PorterStemmer()\n",
        "                stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "                text = ' '.join(filtered_tokens)\n",
        "                stemmed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "                return text, stemmed_text\n",
        "                else:\n",
        "                    return None, None'''"
      ],
      "metadata": {
        "id": "sjcBKNW46O56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "            '''tokens = word_tokenize(original_text)\n",
        "            if language == 'en':\n",
        "                stop_words = set(stopwords.words(\"english\"))\n",
        "            elif language == \"nl\":\n",
        "                stop_words = set(stopwords.words(\"dutch\"))\n",
        "            filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "            words_to_be_removed = [\"cookie\", \"cookies\", \"tracking\", \"privacy\"]\n",
        "            filtered_tokens = [word for word in filtered_tokens if word.lower() not in words_to_be_removed]\n",
        "\n",
        "            stemmer = PorterStemmer()\n",
        "            stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "            text = ' '.join(filtered_tokens)\n",
        "            stemmed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "            return text, stemmed_text'''"
      ],
      "metadata": {
        "id": "ugWkaV-n6VJH"
      }
    }
  ]
}